#!/usr/bin/env python3
"""
TASK 2 (P0) - VALIDATION PERFORMANCE RÃ‰ELLE
Benchmark 1-3 jours avec pipeline COMPLET (â‰¤15 min)

OBJECTIF : Mesurer les vraies performances maintenant que le pipeline HTF est rÃ©parÃ©
"""
import sys
import time
import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import logging

sys.path.insert(0, str(Path(__file__).parent.parent))

# Logging minimal (erreurs uniquement pour perf)
logging.basicConfig(level=logging.ERROR)

from backtest.engine import BacktestEngine
from models.backtest import BacktestConfig

print("="*80)
print("TASK 2 - BENCHMARK PERFORMANCE RÃ‰ELLE")
print("="*80)

# ============================================================================
# PRÃ‰PARATION : Extraire exactement 3 jours de donnÃ©es
# ============================================================================
print("\n[PREP] Extracting 3-day dataset...")
source_path = Path('/app/data/historical/1m/SPY.parquet')
df = pd.read_parquet(source_path)

# Reset index to get datetime column
if 'datetime' not in df.columns:
    df = df.reset_index()

df['datetime'] = pd.to_datetime(df['datetime'], utc=True)
df = df.sort_values('datetime').reset_index(drop=True)

# Extraire 3 jours (juin 2025, 2-4)
start_date = pd.Timestamp('2025-06-02 08:00:00', tz='UTC')
end_date = pd.Timestamp('2025-06-04 20:00:00', tz='UTC')

df_3days = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date)].copy()

if len(df_3days) == 0:
    print(f"âŒ No data found in range {start_date} to {end_date}")
    sys.exit(1)

# Sauvegarder temporairement
temp_path = Path('/tmp/spy_3days_task2.parquet')
df_3days.to_parquet(temp_path, index=False)

print(f"âœ… Dataset prepared: {len(df_3days)} bars")
print(f"   Period: {df_3days['datetime'].min()} â†’ {df_3days['datetime'].max()}")
print(f"   Duration: {(df_3days['datetime'].max() - df_3days['datetime'].min()).total_seconds() / 3600:.1f} hours")

# Configuration
config = BacktestConfig(
    run_name='task2_perf_validation',
    symbols=['SPY'],
    data_paths=[str(temp_path)],
    initial_capital=100000.0,
    trading_mode='AGGRESSIVE',
    trade_types=['DAILY', 'SCALP'],
    output_dir='/app/backend/results'
)

print(f"\nğŸ¯ Target: â‰¤15 minutes execution")
print(f"ğŸ”§ Pipeline: COMPLET (no mocks, no shortcuts)")
print("\n" + "="*80)

# ============================================================================
# PHASE 1 : INITIALISATION
# ============================================================================
print("\n[PHASE 1] Initialisation + Load data...")
t_init_start = time.time()

engine = BacktestEngine(config)
engine.load_data()

t_init = time.time() - t_init_start
print(f"âœ… Init: {t_init:.2f}s")

# Activer instrumentation interne pour tracking
engine.market_state_engine._instrumentation_log = []

# ============================================================================
# PHASE 2 : BACKTEST COMPLET (PIPELINE RÃ‰EL)
# ============================================================================
print("\n[PHASE 2] Running full backtest (engine.run())...")
print("â±ï¸  Tracking: ms/bar, cache hits, market_state calls, HTF stability")
print()

t_run_start = time.time()

# EXÃ‰CUTION COMPLÃˆTE
result = engine.run()

t_run = time.time() - t_run_start

# ============================================================================
# PHASE 3 : EXTRACTION DES MÃ‰TRIQUES
# ============================================================================
print("\n[PHASE 3] Extracting metrics...")

# 3.1 Performance temporelle
total_bars = result.total_bars
avg_ms_per_bar = (t_run * 1000) / total_bars if total_bars > 0 else 0

# Calcul P95 approximatif (pas de per-bar timing, on estime)
# Note: pour un vrai P95, il faudrait mesurer chaque bar individuellement
# Ici on estime via la distribution du cache
p95_estimate_ms = avg_ms_per_bar * 1.5  # Conservative estimate

# 3.2 Market State tracking
market_state_calls = len(engine.market_state_engine._instrumentation_log)

# 3.3 Cache hit rate
cache_stats = engine.market_state_cache.get_stats()
total_requests = cache_stats.get('hits', 0) + cache_stats.get('misses', 0)
cache_hit_rate = (cache_stats.get('hits', 0) / total_requests * 100) if total_requests > 0 else 0

# 3.4 HTF Stability Check (vÃ©rifier que les HTF restent non-nuls)
# On utilise les derniÃ¨res valeurs stockÃ©es dans l'agrÃ©gateur
htf_stable = True
htf_status = {}
for symbol in config.symbols:
    h1_count = len(engine.tf_aggregator.get_candles(symbol, '1h'))
    h4_count = len(engine.tf_aggregator.get_candles(symbol, '4h'))
    d1_count = len(engine.tf_aggregator.get_candles(symbol, '1d'))
    
    htf_status[symbol] = {
        '1h': h1_count,
        '4h': h4_count,
        '1d': d1_count
    }
    
    if h1_count == 0 or h4_count == 0 or d1_count == 0:
        htf_stable = False

# 3.5 Sanity fonctionnelle
trades_generated = result.total_trades
has_errors = False  # Si on arrive ici, pas d'exception

# ============================================================================
# PHASE 4 : RAPPORT FINAL (MÃ‰TRIQUES BRUTES UNIQUEMENT)
# ============================================================================
print("\n" + "="*80)
print("TASK 2 - RÃ‰SULTATS BRUTS")
print("="*80)

print("\nğŸ“Š PERFORMANCE TEMPORELLE")
print(f"  â€¢ Total execution time: {t_run:.2f}s ({t_run/60:.2f} min)")
print(f"  â€¢ Total bars processed: {total_bars}")
print(f"  â€¢ ms/bar (avg): {avg_ms_per_bar:.2f} ms")
print(f"  â€¢ ms/bar (P95 estimate): {p95_estimate_ms:.2f} ms")
print(f"  â€¢ Bars/second: {total_bars/t_run:.1f}")

print("\nğŸ“Š MARKET STATE")
print(f"  â€¢ create_market_state() calls: {market_state_calls}")
print(f"  â€¢ Cache requests: {total_requests}")
print(f"  â€¢ Cache hit rate: {cache_hit_rate:.1f}%")
print(f"  â€¢ Cache hits: {cache_stats.get('hits', 0)}")
print(f"  â€¢ Cache misses: {cache_stats.get('misses', 0)}")

print("\nğŸ“Š HTF STABILITY")
print(f"  â€¢ HTF pipeline stable: {'âœ… YES' if htf_stable else 'âŒ NO'}")
for symbol, counts in htf_status.items():
    print(f"  â€¢ {symbol}: 1h={counts['1h']}, 4h={counts['4h']}, 1d={counts['1d']}")

print("\nğŸ“Š SANITY FONCTIONNELLE")
print(f"  â€¢ Trades generated: {trades_generated}")
print(f"  â€¢ Exceptions/Errors: {'âŒ YES' if has_errors else 'âœ… NO'}")
print(f"  â€¢ Win rate: {result.winrate:.1f}%")
print(f"  â€¢ Expectancy R: {result.expectancy_r:.3f}R")

print("\n" + "="*80)
print("VERDICT")
print("="*80)

# Validation des contraintes
constraint_time = t_run <= 900  # â‰¤15 min
constraint_htf = htf_stable
constraint_no_errors = not has_errors

print(f"  âœ“ Time constraint (â‰¤15 min): {'âœ… PASS' if constraint_time else 'âŒ FAIL'} ({t_run/60:.2f} min)")
print(f"  âœ“ HTF pipeline valid: {'âœ… PASS' if constraint_htf else 'âŒ FAIL'}")
print(f"  âœ“ No errors/exceptions: {'âœ… PASS' if constraint_no_errors else 'âŒ FAIL'}")

all_pass = constraint_time and constraint_htf and constraint_no_errors
print(f"\n{'âœ… ALL CONSTRAINTS MET' if all_pass else 'âŒ CONSTRAINTS FAILED'}")

print("\n" + "="*80)

# Sauvegarder les mÃ©triques brutes en JSON
metrics = {
    "timestamp": datetime.now().isoformat(),
    "config": {
        "symbols": config.symbols,
        "trading_mode": config.trading_mode,
        "trade_types": config.trade_types,
        "period": f"{result.start_date.isoformat()} to {result.end_date.isoformat()}"
    },
    "performance": {
        "total_execution_seconds": t_run,
        "total_bars": total_bars,
        "ms_per_bar_avg": avg_ms_per_bar,
        "ms_per_bar_p95_estimate": p95_estimate_ms,
        "bars_per_second": total_bars/t_run if t_run > 0 else 0
    },
    "market_state": {
        "calls_total": market_state_calls,
        "cache_requests": total_requests,
        "cache_hit_rate_pct": cache_hit_rate,
        "cache_hits": cache_stats.get('hits', 0),
        "cache_misses": cache_stats.get('misses', 0)
    },
    "htf_stability": {
        "stable": htf_stable,
        "status": htf_status
    },
    "sanity": {
        "trades_generated": trades_generated,
        "has_errors": has_errors,
        "winrate": result.winrate,
        "expectancy_r": result.expectancy_r
    },
    "constraints": {
        "time_constraint_pass": constraint_time,
        "htf_constraint_pass": constraint_htf,
        "error_constraint_pass": constraint_no_errors,
        "all_pass": all_pass
    }
}

output_path = Path('/app/backend/results/task2_metrics.json')
with output_path.open('w') as f:
    json.dump(metrics, f, indent=2)

print(f"\nğŸ“ Metrics saved: {output_path}")
print("\n" + "="*80)
