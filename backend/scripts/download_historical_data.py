"""
Script de tÃ©lÃ©chargement de donnÃ©es historiques M1 pour backtesting
StratÃ©gie : 30 derniers jours complets â†’ dÃ©coupage en blocs de 6 jours
"""
import yfinance as yf
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# CrÃ©er les rÃ©pertoires de donnÃ©es
DATA_DIR = Path("/app/data/historical")
DATA_DIR_1M = DATA_DIR / "1m"
DATA_DIR_5M = DATA_DIR / "5m"

DATA_DIR_1M.mkdir(parents=True, exist_ok=True)
DATA_DIR_5M.mkdir(parents=True, exist_ok=True)

SYMBOLS = ["SPY", "QQQ"]


def download_last_n_days_1m(symbol: str, days: int = 7):
    """
    TÃ©lÃ©charge les N derniers jours en 1 minute
    yfinance limite : maximum 7-8 jours par requÃªte
    
    Args:
        symbol: Ticker (SPY ou QQQ)
        days: Nombre de jours (max 7 recommandÃ©)
    
    Returns:
        DataFrame avec les donnÃ©es, ou None si Ã©chec
    """
    try:
        logger.info(f"ðŸ“¥ TÃ©lÃ©chargement {symbol} : {days} derniers jours (1min)")
        
        ticker = yf.Ticker(symbol)
        df = ticker.history(period=f"{days}d", interval="1m")
        
        if df.empty:
            logger.warning(f"Aucune donnÃ©e pour {symbol}")
            return None
        
        # Nettoyer les donnÃ©es
        df = df.reset_index()
        df.columns = [col.lower() for col in df.columns]
        
        # S'assurer que Datetime est bien datetime
        if 'datetime' in df.columns:
            df['datetime'] = pd.to_datetime(df['datetime'])
        elif 'date' in df.columns:
            df['datetime'] = pd.to_datetime(df['date'])
            df = df.drop('date', axis=1)
        
        # Colonnes nÃ©cessaires
        required_cols = ['datetime', 'open', 'high', 'low', 'close', 'volume']
        df = df[required_cols]
        
        # Trier par datetime
        df = df.sort_values('datetime').reset_index(drop=True)
        
        logger.info(f"âœ… {symbol} : {len(df)} lignes")
        logger.info(f"   PÃ©riode : {df['datetime'].min()} Ã  {df['datetime'].max()}")
        
        return df
    
    except Exception as e:
        logger.error(f"âŒ Erreur tÃ©lÃ©chargement {symbol} : {e}")
        return None


def download_multiple_7day_blocks(symbol: str, num_blocks: int = 4):
    """
    TÃ©lÃ©charge plusieurs blocs de 7 jours en dÃ©calant dans le temps
    
    StratÃ©gie : TÃ©lÃ©charger les derniers 7j, puis 7j avant, etc.
    Limitation yfinance : on ne peut avoir que les 30 derniers jours total
    
    Args:
        symbol: Ticker
        num_blocks: Nombre de blocs de 7 jours Ã  tÃ©lÃ©charger (max 4 pour 28 jours)
    
    Returns:
        List de DataFrames
    """
    all_dfs = []
    
    for i in range(num_blocks):
        # Calculer la pÃ©riode
        end_date = datetime.now() - timedelta(days=i*7)
        start_date = end_date - timedelta(days=7)
        
        logger.info(f"\nðŸ“¦ Bloc {i+1}/{num_blocks} : {start_date.strftime('%Y-%m-%d')} â†’ {end_date.strftime('%Y-%m-%d')}")
        
        try:
            ticker = yf.Ticker(symbol)
            df = ticker.history(
                start=start_date.strftime('%Y-%m-%d'),
                end=end_date.strftime('%Y-%m-%d'),
                interval="1m"
            )
            
            if df.empty:
                logger.warning(f"   Bloc {i+1} vide")
                continue
            
            # Nettoyer
            df = df.reset_index()
            df.columns = [col.lower() for col in df.columns]
            
            if 'datetime' in df.columns:
                df['datetime'] = pd.to_datetime(df['datetime'])
            elif 'date' in df.columns:
                df['datetime'] = pd.to_datetime(df['date'])
                df = df.drop('date', axis=1)
            
            required_cols = ['datetime', 'open', 'high', 'low', 'close', 'volume']
            df = df[required_cols]
            df = df.sort_values('datetime').reset_index(drop=True)
            
            logger.info(f"   âœ… {len(df)} lignes : {df['datetime'].min()} â†’ {df['datetime'].max()}")
            all_dfs.append(df)
        
        except Exception as e:
            logger.error(f"   âŒ Erreur bloc {i+1} : {e}")
            continue
    
    return all_dfs


def analyze_block_characteristics(df: pd.DataFrame, block_name: str):
    """
    Analyse les caractÃ©ristiques d'un bloc de donnÃ©es
    
    Returns:
        Dict avec mÃ©triques (volatilitÃ©, trend, range, etc.)
    """
    # Calculer ATR (Average True Range) simplifiÃ©
    df['tr'] = df[['high', 'low']].apply(lambda x: x['high'] - x['low'], axis=1)
    atr = df['tr'].mean()
    
    # Calculer le range total
    price_range = df['high'].max() - df['low'].min()
    
    # Calculer le mouvement directionnel
    first_close = df['close'].iloc[0]
    last_close = df['close'].iloc[-1]
    directional_move = last_close - first_close
    directional_pct = (directional_move / first_close) * 100
    
    # DÃ©tecter trend vs range
    # Si le prix bouge plus de 2% de maniÃ¨re directionnelle = trend
    # Sinon = range/chop
    is_trend = abs(directional_pct) > 2.0
    trend_type = "uptrend" if directional_pct > 0 else "downtrend" if directional_pct < 0 else "neutral"
    
    # VolatilitÃ© relative (ATR / prix moyen)
    avg_price = df['close'].mean()
    volatility_pct = (atr / avg_price) * 100
    
    return {
        'name': block_name,
        'start': df['datetime'].min(),
        'end': df['datetime'].max(),
        'bars': len(df),
        'atr': round(atr, 2),
        'volatility_pct': round(volatility_pct, 3),
        'price_range': round(price_range, 2),
        'directional_move': round(directional_move, 2),
        'directional_pct': round(directional_pct, 2),
        'is_trend': is_trend,
        'trend_type': trend_type,
        'first_close': round(first_close, 2),
        'last_close': round(last_close, 2)
    }


def split_into_6day_blocks(df: pd.DataFrame, symbol: str):
    """
    DÃ©coupe un DataFrame en blocs de 6 jours de trading consÃ©cutifs
    
    Args:
        df: DataFrame avec colonnes datetime, open, high, low, close, volume
        symbol: Ticker (pour nommage des fichiers)
    
    Returns:
        List de dicts avec info sur chaque bloc
    """
    logger.info(f"\nðŸ”ª DÃ©coupage en blocs de 6 jours pour {symbol}")
    
    # Grouper par jour
    df['date'] = df['datetime'].dt.date
    days = df['date'].unique()
    
    logger.info(f"   Total jours de trading : {len(days)}")
    
    # CrÃ©er des blocs de 6 jours consÃ©cutifs
    blocks = []
    block_num = 1
    
    for i in range(0, len(days), 6):
        block_days = days[i:i+6]
        
        if len(block_days) < 5:  # On veut au moins 5 jours
            logger.info(f"   Bloc {block_num} ignorÃ© (seulement {len(block_days)} jours)")
            continue
        
        # Extraire les donnÃ©es de ce bloc
        block_df = df[df['date'].isin(block_days)].copy()
        block_df = block_df.drop('date', axis=1)
        
        # Nom du bloc
        start_date = block_df['datetime'].min().strftime('%Y%m%d')
        end_date = block_df['datetime'].max().strftime('%Y%m%d')
        block_name = f"block{block_num}_{start_date}_{end_date}"
        
        # Analyser les caractÃ©ristiques
        characteristics = analyze_block_characteristics(block_df, block_name)
        
        # Sauvegarder le bloc
        filename = f"{symbol.lower()}_1m_{block_name}.parquet"
        filepath = DATA_DIR_1M / filename
        block_df.to_parquet(filepath, index=False)
        
        blocks.append({
            'symbol': symbol,
            'block_num': block_num,
            'filename': filename,
            'filepath': str(filepath),
            **characteristics
        })
        
        logger.info(f"   âœ… Bloc {block_num} : {start_date} â†’ {end_date}")
        logger.info(f"      Bars: {len(block_df)}, Type: {characteristics['trend_type']}, Vol: {characteristics['volatility_pct']:.3f}%")
        
        block_num += 1
    
    return blocks


def download_and_split_all():
    """
    TÃ©lÃ©charge plusieurs blocs de 7 jours en 1min pour SPY et QQQ
    Puis les dÃ©coupe en blocs de 6 jours utilisables pour backtest
    """
    logger.info("="*80)
    logger.info("TÃ‰LÃ‰CHARGEMENT DONNÃ‰ES M1 - STRATÃ‰GIE MULTI-BLOCS 7 JOURS")
    logger.info("="*80)
    logger.info("Limite yfinance : 7-8 jours max par requÃªte")
    logger.info("Solution : TÃ©lÃ©charger 4 blocs de 7 jours (28 jours total)")
    logger.info("="*80)
    
    all_blocks = []
    
    for symbol in SYMBOLS:
        logger.info(f"\n{'='*80}")
        logger.info(f"SYMBOLE : {symbol}")
        logger.info(f"{'='*80}")
        
        # TÃ©lÃ©charger 4 blocs de 7 jours
        dfs = download_multiple_7day_blocks(symbol, num_blocks=4)
        
        if not dfs:
            logger.error(f"âŒ Aucune donnÃ©e pour {symbol}")
            continue
        
        # Combiner tous les blocs
        logger.info(f"\nðŸ”— Combinaison de {len(dfs)} blocs...")
        combined_df = pd.concat(dfs, ignore_index=True)
        combined_df = combined_df.sort_values('datetime').reset_index(drop=True)
        
        # Supprimer les doublons Ã©ventuels
        combined_df = combined_df.drop_duplicates(subset=['datetime'], keep='first')
        
        logger.info(f"âœ… Total combinÃ© : {len(combined_df)} lignes")
        logger.info(f"   PÃ©riode : {combined_df['datetime'].min()} â†’ {combined_df['datetime'].max()}")
        
        # Sauvegarder le fichier combinÃ©
        filename = f"{symbol.lower()}_1m_combined.parquet"
        filepath = DATA_DIR_1M / filename
        combined_df.to_parquet(filepath, index=False)
        logger.info(f"   SauvegardÃ© : {filepath}")
        
        # DÃ©couper en blocs de 6 jours
        blocks = split_into_6day_blocks(combined_df, symbol)
        all_blocks.extend(blocks)
    
    return all_blocks


def verify_data_integrity():
    """VÃ©rifie l'intÃ©gritÃ© des donnÃ©es tÃ©lÃ©chargÃ©es"""
    logger.info("\n" + "="*80)
    logger.info("VÃ‰RIFICATION DE L'INTÃ‰GRITÃ‰ DES DONNÃ‰ES")
    logger.info("="*80)
    
    files = sorted(DATA_DIR_1M.glob("*.parquet"))
    
    # Exclure le fichier de rÃ©sumÃ©
    files = [f for f in files if 'summary' not in f.name]
    
    if not files:
        logger.error("âŒ Aucun fichier trouvÃ© !")
        return False
    
    all_ok = True
    
    for filepath in files:
        try:
            df = pd.read_parquet(filepath)
            
            # VÃ©rifications
            checks = {
                "Non vide": len(df) > 0,
                "Colonnes OK": all(col in df.columns for col in ['datetime', 'open', 'high', 'low', 'close', 'volume']),
                "Pas de NaN": not df[['open', 'high', 'low', 'close']].isnull().any().any(),
                "High >= Low": (df['high'] >= df['low']).all(),
                "Close dans range": ((df['close'] >= df['low']) & (df['close'] <= df['high'])).all()
            }
            
            all_checks_passed = all(checks.values())
            
            status = "âœ…" if all_checks_passed else "âŒ"
            
            if not all_checks_passed:
                logger.info(f"{status} {filepath.name}")
                for check_name, passed in checks.items():
                    if not passed:
                        logger.warning(f"   âš  {check_name} : Ã‰CHEC")
                all_ok = False
        
        except Exception as e:
            logger.error(f"âŒ Erreur lecture {filepath.name} : {e}")
            all_ok = False
    
    return all_ok


def generate_blocks_summary(blocks):
    """GÃ©nÃ¨re un rÃ©sumÃ© des blocs tÃ©lÃ©chargÃ©s"""
    if not blocks:
        logger.error("Aucun bloc crÃ©Ã©")
        return
    
    logger.info("\n" + "="*80)
    logger.info("ðŸ“Š RÃ‰SUMÃ‰ DES BLOCS CRÃ‰Ã‰S")
    logger.info("="*80)
    
    # Grouper par symbole
    spy_blocks = [b for b in blocks if b['symbol'] == 'SPY']
    qqq_blocks = [b for b in blocks if b['symbol'] == 'QQQ']
    
    logger.info(f"\nSPY : {len(spy_blocks)} blocs")
    logger.info(f"QQQ : {len(qqq_blocks)} blocs")
    
    # Analyser les contextes
    logger.info("\n" + "="*80)
    logger.info("ðŸŽ¯ ANALYSE DES CONTEXTES")
    logger.info("="*80)
    
    for symbol in ['SPY', 'QQQ']:
        symbol_blocks = [b for b in blocks if b['symbol'] == symbol]
        
        if not symbol_blocks:
            continue
        
        logger.info(f"\n{symbol}:")
        
        # Compter les types
        trends = [b for b in symbol_blocks if b['is_trend']]
        ranges = [b for b in symbol_blocks if not b['is_trend']]
        uptrends = [b for b in symbol_blocks if b['trend_type'] == 'uptrend']
        downtrends = [b for b in symbol_blocks if b['trend_type'] == 'downtrend']
        
        logger.info(f"  Trends : {len(trends)} ({len(uptrends)} up, {len(downtrends)} down)")
        logger.info(f"  Ranges : {len(ranges)}")
        
        # VolatilitÃ© moyenne
        avg_vol = np.mean([b['volatility_pct'] for b in symbol_blocks])
        logger.info(f"  VolatilitÃ© moyenne : {avg_vol:.3f}%")
        
        # Lister les blocs
        logger.info(f"\n  DÃ©tail des blocs :")
        for b in symbol_blocks:
            context = f"{b['trend_type']}" if b['is_trend'] else "range/chop"
            logger.info(f"    â€¢ {b['name']}: {context}, vol={b['volatility_pct']:.3f}%, move={b['directional_pct']:+.2f}%")
    
    # Sauvegarder un CSV de rÃ©sumÃ©
    df_summary = pd.DataFrame(blocks)
    summary_path = DATA_DIR_1M / "blocks_summary.csv"
    df_summary.to_csv(summary_path, index=False)
    logger.info(f"\nðŸ“„ RÃ©sumÃ© sauvegardÃ© : {summary_path}")


if __name__ == "__main__":
    # TÃ©lÃ©charger et dÃ©couper
    blocks = download_and_split_all()
    
    if blocks:
        # VÃ©rifier l'intÃ©gritÃ©
        integrity_ok = verify_data_integrity()
        
        # GÃ©nÃ©rer le rÃ©sumÃ©
        generate_blocks_summary(blocks)
        
        if integrity_ok:
            logger.info("\n" + "="*80)
            logger.info("ðŸŽ‰ TÃ‰LÃ‰CHARGEMENT ET DÃ‰COUPAGE TERMINÃ‰S AVEC SUCCÃˆS !")
            logger.info("="*80)
            logger.info(f"\nðŸ“‚ Fichiers dans : {DATA_DIR_1M}")
            logger.info(f"   Total blocs : {len(blocks)}")
        else:
            logger.error("\nâš  TÃ©lÃ©chargement terminÃ© mais des problÃ¨mes d'intÃ©gritÃ© dÃ©tectÃ©s")
    else:
        logger.error("\nâŒ Ã‰chec du tÃ©lÃ©chargement")
